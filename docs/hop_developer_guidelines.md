
# ğŸ› ï¸ Developer Guidelines fÃ¼r Data Engineers â€“ Apache Hop

Diese Guidelines sollen sicherstellen, dass du mit Apache Hop standardisiert, wartbar und teamkompatibel arbeitest â€“ im Kontext von Data Mesh und produktionsreifen Datenpipelines.

---

## ğŸ“¦ 1. Projektstruktur

- Verwende pro Datenprodukt ein **eigenes Hop-Projekt** (z.â€¯B. `customer_profile_project`).
- Gliedere logisch:
  - `pipelines/` fÃ¼r Hauptprozesse
  - `workflows/` fÃ¼r Steuerung, Validierung, Deployment
  - `metadata/` fÃ¼r Verbindungen, Umgebungen, Parameter

---

## ğŸ§­ 2. Naming Conventions

| Element         | Konvention                            | Beispiel                         |
|----------------|----------------------------------------|----------------------------------|
| Pipelines       | `p_<modul>_<funktion>`                | `p_orders_enrich_customers`      |
| Workflows       | `wf_<modul>_<zweck>`                  | `wf_orders_full_load`            |
| Transformations | `t_<beschreibung>`                    | `t_join_customer_orders`         |
| Files / Folder  | `lower_snake_case`                    | `metadata/databases/postgres.json` |

---

## âš™ï¸ 3. Technische Gestaltung

- Verwende **Parameter** statt harter Pfade (`${ENVIRONMENT}`, `${DB_CONNECTION}`)
- Vermeide globale Variablen in Pipelines â€“ nutze Ã¼bergebbare Werte
- Alle Datenbankverbindungen gehÃ¶ren in `metadata/databases/`
- Gruppiere TransformationsblÃ¶cke logisch mit Annotationen

---

## ğŸ” 4. Modulare Pipelines

- Baue **wiederverwendbare Subpipelines** (z.â€¯B. `clean_customer_data.hpl`)
- Trenne **Logik (Transformation)** und **Steuerung (Workflow)** klar
- Ein Workflow = genau **ein Jobziel** (Load, Sync, Export etc.)

---

## ğŸ§ª 5. Testing & Validierung

- Verwende **"Assert Rows"** zur QualitÃ¤tskontrolle in Pipelines
- Automatisiere Tests Ã¼ber CLI (`hop-run`) oder CI/CD-Tools
- FÃ¼ge Validierungen am Anfang jeder Pipeline ein (Schema Check, Pflichtfelder)

---

## ğŸ“¦ 6. Deployment & Umgebungen

- Definiere Umgebungen in `metadata/environment/` (`dev`, `test`, `prod`)
- Deployment per CLI (`hop-run`, `hop-server`) oder orchestriert (z.â€¯B. Airflow, GitLab CI)
- Halte Umgebungsvariablen konsistent und dokumentiert

---

## ğŸ”’ 7. Sicherheit & Datenschutz

- Maskiere sensible Felder direkt in Pipelines (z.â€¯B. Hash von E-Mails)
- Keine PasswÃ¶rter im Klartext â€“ verwende Parameter + Secret Management
- DSGVO-relevante Pipelines klar kennzeichnen (`[PII]` im Namen)

---

## ğŸ“ˆ 8. Logging & Monitoring

- Aktiviere detailliertes Logging (`log_level: Basic` oder `Detailed`)
- Leite Logs in zentrale Systeme (z.â€¯B. ELK, Grafana via Log-Dateien)
- Verwende eindeutige Run-IDs und schreibe Prozessmetadaten in Audit-Tabellen

---

## ğŸ”— 9. Schnittstellen & Integration

- EingÃ¤nge: Datenbank, Flat Files, Kafka, REST, S3
- AusgÃ¤nge: Datenbank, S3, REST, Message Queue, Kafka
- Transformationsschritte klar dokumentieren (Schema, Erwartung, Typen) in Notes beschreiben

---

## ğŸ“š 10. Dokumentation & Versionierung

- Jede Pipeline & Workflow braucht eine `.md` Beschreibung (Zweck, Input, Output, Besonderheiten)
- Versioniere Pipelines in Git (`.hpl`, `.hwf`, `.json`) â€“ keine BinÃ¤rdateien!
- Git-Konventionen:
  - Feature-Branches: `feature/<beschreibung>`
  - Commits: `ADD`, `FIX`, `REFACTOR`

---

## ğŸ§  Tipps

- Nutze `Hop Gui` fÃ¼r visuelles Debugging â€“ aber speichere sauber versionierte Dateien
- Erzeuge `preview`-Schritte fÃ¼r komplexe Transformationsketten
- Vermeide unnÃ¶tige Lookups â€“ arbeite mit vorbereiteten Joins und Caching

---

## ğŸ”— NÃ¼tzliche Links

- [Apache Hop Doku](https://hop.apache.org/manual/latest/)
- [CLI Referenz](https://hop.apache.org/manual/latest/cli/hop-run/)
- [GitHub Best Practices](https://github.com/apache/hop)

---

Diese Richtlinien stellen sicher, dass Apache Hop in einem Data Mesh-Umfeld **skalierbar**, **teamfÃ¤hig** und **produktionstauglich** eingesetzt wird.
