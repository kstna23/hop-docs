{"config":{"lang":["de"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\uddf0 Was ist Apache Hop?","text":"<p>Apache Hop (Hop Orchestration Platform) ist ein Open-Source-Datenintegrations- und Orchestrierungs-Framework, das es Data Engineers erm\u00f6glicht, grafisch oder skriptbasiert ETL/ELT-Pipelines zu entwickeln, zu verwalten und auszuf\u00fchren \u2013 sowohl lokal als auch in produktiven Umgebungen.</p>"},{"location":"#architekturuberblick","title":"\ud83d\udcca Architektur\u00fcberblick","text":"<p>Apache Hop besteht aus:</p> <ul> <li>Hop GUI: Lokales grafisches Entwicklungstool</li> <li>hop-run: Kommandozeilentool zum Ausf\u00fchren von Pipelines und Workflows</li> <li>Hop Server: REST-basiertes API f\u00fcr Remote-Ausf\u00fchrungen und Orchestrierung</li> <li>Metadata System: F\u00fcr Umgebungen, Verbindungen, Variablen etc.</li> </ul>"},{"location":"#kernfunktionalitaten","title":"\ud83d\ude80 Kernfunktionalit\u00e4ten","text":"Feature Beschreibung \ud83e\uddf1 Modular Pipelines (Transformationen) &amp; Workflows (Steuerlogik) getrennt modellierbar \ud83d\udda5\ufe0f GUI + CLI Entwickeln mit Hop GUI, Ausf\u00fchren mit <code>hop-run</code> oder in Docker/K8s \ud83d\udd01 Wiederverwendbar Subpipelines, Parameter, Umgebungsvariablen \ud83d\udcbe Breite Datenquellen DBs, Files, Kafka, REST, Cloud Storage, etc. \ud83d\udcc4 Metadatengetrieben Alles als Datei im JSON/YAML-Format versionierbar \ud83d\udd0c Erweiterbar Pluginsystem f\u00fcr eigene Schritte und Tools"},{"location":"#warum-apache-hop","title":"\ud83c\udfaf Warum Apache Hop?","text":"<p>Apache Hop ist ideal f\u00fcr moderne Data Engineering Workflows, weil es:</p> <ul> <li>visuelle Entwicklung mit DevOps-Kompatibilit\u00e4t verbindet</li> <li>mit DataOps &amp; Data Mesh-Paradigmen harmoniert</li> <li>eine leichtgewichtige und kostenfreie, nach Apache-lizenzierte Alternative zu Airflow (mehr Scheduling, Orchestration), Talend oder Informatica bietet</li> </ul>"},{"location":"#typische-einsatzszenarien","title":"\ud83d\udce6 Typische Einsatzszenarien","text":"<ul> <li>ETL/ELT-Prozesse im DWH oder Data Lake</li> <li>Datenbereinigung &amp; Anreicherung</li> <li>Orchestrierung komplexer Datenworkflows</li> <li>Batch- &amp; Streaming-Integration</li> <li>Vorbereitung von Daten f\u00fcr ML/Analytics</li> <li>Bereitstellen von Daten via API</li> </ul>"},{"location":"#weiterfuhrende-links","title":"\ud83d\udd17 Weiterf\u00fchrende Links","text":"<ul> <li>\ud83c\udf10 Offizielle Website: https://hop.apache.org</li> <li>\ud83d\udcda Dokumentation: https://hop.apache.org/manual/latest/</li> <li>\ud83d\udee0\ufe0f GitHub Repo: https://github.com/apache/hop</li> <li>\ud83d\udcac Community Slack: https://hop.apache.org/community/slack/</li> <li>\ud83e\uddea Tutorials &amp; Beispiele: https://hop.apache.org/docs/</li> </ul>"},{"location":"#empfehlung","title":"\ud83e\udde0 Empfehlung","text":"<p>Wenn du nach einem Open-Source-Werkzeug suchst, das dich bei der Erstellung von wiederholbaren, testbaren, modularen und produktionsreifen Datenpipelines unterst\u00fctzt \u2013 und gleichzeitig mit modernen Prinzipien wie Data Mesh oder DataOps harmoniert \u2013 ist Apache Hop eine ausgezeichnete Wahl.</p>"},{"location":"datenproduktbeschreibung_technisch/","title":"\ud83d\udce6 Datenproduktbeschreibung: <code>your_data_product_name</code>","text":""},{"location":"datenproduktbeschreibung_technisch/#1-fachliche-domane","title":"\ud83e\udded 1. Fachliche Dom\u00e4ne","text":"<ul> <li>Dom\u00e4ne: <code>z.\u202fB. Customer Management / Produktion / Vertrieb</code></li> <li>Produktverantwortliches Team: <code>Teamname oder Abteilung</code></li> <li>Kontakt: <code>team@example.com</code></li> </ul>"},{"location":"datenproduktbeschreibung_technisch/#2-zweck-mehrwert","title":"\ud83c\udfaf 2. Zweck &amp; Mehrwert","text":"<p>Beschreibe in 2\u20134 S\u00e4tzen, warum es dieses Datenprodukt gibt und welchen gesch\u00e4ftlichen oder analytischen Nutzen es bringt.</p> <p>Beispiel: Dieses Datenprodukt liefert einen konsolidierten Blick auf alle Kundenprofile, inklusive Kaufverhalten, Loyalty-Status und Lifetime Value. Es unterst\u00fctzt Kampagnen, Segmentierungen und personalisierte Empfehlungen.</p>"},{"location":"datenproduktbeschreibung_technisch/#3-zielgruppen-konsumenten","title":"\ud83d\udc64 3. Zielgruppen &amp; Konsumenten","text":"<ul> <li>Fachabteilungen: <code>z.\u202fB. Marketing, Vertrieb, Service</code></li> <li>Datenrollen: <code>z.\u202fB. Analyst*innen, Data Scientists, ML Engineers</code></li> <li>Systeme: <code>z.\u202fB. Recommendation Engine, Dashboards</code></li> </ul>"},{"location":"datenproduktbeschreibung_technisch/#4-hauptinhalte-des-datenprodukts","title":"\ud83d\udcc4 4. Hauptinhalte des Datenprodukts","text":"Attribut Beschreibung <code>customer_id</code> Eindeutige Kundenkennung <code>loyalty_status</code> Aktueller Status im Treueprogramm <code>lifetime_value</code> Berechneter Gesamtwert eines Kunden <code>first_purchase_date</code> Zeitpunkt der ersten Bestellung <code>region</code> Zuordnung zum geografischen Markt"},{"location":"datenproduktbeschreibung_technisch/#5-datenschutz-sicherheit","title":"\ud83d\udd12 5. Datenschutz &amp; Sicherheit","text":"<ul> <li>Personenbezogene Daten enthalten? <code>Ja / Nein</code></li> <li>DSGVO-Relevant? <code>Ja / Nein</code></li> <li>Sichtbar f\u00fcr: <code>Nur autorisierte Rollen / Nur anonymisiert / Offen intern</code></li> <li>Spezielle Zugriffsrichtlinien: <code>z.\u202fB. PII-Maskierung, L\u00f6schpflichten</code></li> </ul>"},{"location":"datenproduktbeschreibung_technisch/#6-aktualitat-qualitat","title":"\ud83d\udcc8 6. Aktualit\u00e4t &amp; Qualit\u00e4t","text":"<ul> <li>Aktualisierungsfrequenz: <code>z.\u202fB. t\u00e4glich, st\u00fcndlich, in Echtzeit</code></li> <li>Datenerfassungszeitraum: <code>z.\u202fB. ab 2020 bis heute</code></li> <li>Datenqualit\u00e4t: <code>z.\u202fB. &gt; 99% Felder vollst\u00e4ndig, automatisierte Checks</code></li> <li>Letzter erfolgreicher Lauf: <code>YYYY-MM-DD HH:MM</code></li> </ul>"},{"location":"datenproduktbeschreibung_technisch/#7-technische-details","title":"\u2699\ufe0f 7. Technische Details","text":"<ul> <li>Quellsysteme: <code>z.\u202fB. SAP CRM, Kafka Events, Web-Tracking</code></li> <li>ETL/ELT-Werkzeug: <code>z.\u202fB. dbt, Airflow, Talend, Azure Data Factory</code></li> <li>Datenbank/Ziellager: <code>z.\u202fB. Snowflake, BigQuery, Delta Lake</code></li> <li>Verarbeitungstechnologie: <code>z.\u202fB. Spark, SQL, dbt</code></li> <li>Transformationstyp: <code>Batch / Streaming / Micro-Batch</code></li> <li>Wichtige Join-Operationen: </li> <li><code>JOIN customer_events USING (customer_id)</code></li> <li><code>LEFT JOIN region_mapping ON zip_code = region_zip</code></li> <li>Businesslogik/Filter: </li> <li>Nur aktive Kunden (Status = 'ACTIVE')  </li> <li>Umsatz &gt; 0 innerhalb der letzten 12 Monate</li> </ul>"},{"location":"datenproduktbeschreibung_technisch/#8-speicherstruktur","title":"\ud83d\udcc2 8. Speicherstruktur","text":"<ul> <li>Physische Ablage / Pfad: <code>z.\u202fB. s3://data/products/customer_profile/</code></li> <li>Tabellenname (Warehouse): <code>z.\u202fB. prod.customer_profile_v1</code></li> <li>Schema-Versionierung: <code>v1, v2 \u2026</code></li> <li>Datenretention: <code>z.\u202fB. 180 Tage (Query-f\u00e4hig), Archiv 2 Jahre</code></li> </ul>"},{"location":"datenproduktbeschreibung_technisch/#9-technische-dokumentation-zugang","title":"\ud83d\udcce 9. Technische Dokumentation &amp; Zugang","text":"<ul> <li>Schema-Dokumentation</li> <li>ETL-Pipeline-Definition</li> <li>Transformation in dbt</li> <li>Data Catalog-Eintrag</li> <li>Monitoring Dashboard (z.\u202fB. Grafana)</li> <li>Ansprechpartner: <code>tech-lead@example.com</code></li> </ul>"},{"location":"datenproduktbeschreibung_technisch/#10-herkunft-lineage","title":"\ud83d\uddfa\ufe0f 10. Herkunft &amp; Lineage","text":"<ul> <li>Quellen: <code>CRM-Events, E-Commerce-Orders, Loyalty-Service</code></li> <li>ETL-Fluss: <code>CRM \u2192 Raw Layer \u2192 Staging \u2192 Aggregation \u2192 Produkt</code></li> <li>Lineage-Tool: <code>OpenLineage, Marquez, DataHub</code></li> <li>Change Impact: dokumentiert im GitHub Repo (Contract-Tests)</li> </ul>"},{"location":"datenproduktbeschreibung_technisch/#11-tests-qualitatssicherung","title":"\ud83e\uddea 11. Tests &amp; Qualit\u00e4tssicherung","text":"<ul> <li>Datenqualit\u00e4tsregeln: <code>z.\u202fB. NULL-Anteil &lt; 1 %, IDs eindeutig</code></li> <li>Validierungsframework: <code>Great Expectations, SodaSQL, dbt tests</code></li> <li>Monitoring: z.\u202fB. Alerts bei Datenvolumenabweichung &gt; 20%</li> </ul>"},{"location":"hop_developer_guidelines/","title":"\ud83d\udee0\ufe0f Developer Guidelines f\u00fcr Data Engineers \u2013 Apache Hop","text":"<p>Diese Guidelines sollen sicherstellen, dass du mit Apache Hop standardisiert, wartbar und teamkompatibel arbeitest \u2013 im Kontext von Data Mesh und produktionsreifen Datenpipelines.</p>"},{"location":"hop_developer_guidelines/#1-projektstruktur","title":"\ud83d\udce6 1. Projektstruktur","text":"<ul> <li>Verwende pro Datenprodukt ein eigenes Hop-Projekt (z.\u202fB. <code>customer_profile_project</code>).</li> <li>Gliedere logisch:</li> <li><code>pipelines/</code> f\u00fcr Hauptprozesse</li> <li><code>workflows/</code> f\u00fcr Steuerung, Validierung, Deployment</li> <li><code>metadata/</code> f\u00fcr Verbindungen, Umgebungen, Parameter</li> </ul>"},{"location":"hop_developer_guidelines/#2-naming-conventions","title":"\ud83e\udded 2. Naming Conventions","text":"Element Konvention Beispiel Pipelines <code>p_&lt;modul&gt;_&lt;funktion&gt;</code> <code>p_orders_enrich_customers</code> Workflows <code>wf_&lt;modul&gt;_&lt;zweck&gt;</code> <code>wf_orders_full_load</code> Transformations <code>t_&lt;beschreibung&gt;</code> <code>t_join_customer_orders</code> Files / Folder <code>lower_snake_case</code> <code>metadata/databases/postgres.json</code>"},{"location":"hop_developer_guidelines/#3-technische-gestaltung","title":"\u2699\ufe0f 3. Technische Gestaltung","text":"<ul> <li>Verwende Parameter statt harter Pfade (<code>${ENVIRONMENT}</code>, <code>${DB_CONNECTION}</code>)</li> <li>Vermeide globale Variablen in Pipelines \u2013 nutze \u00fcbergebbare Werte</li> <li>Alle Datenbankverbindungen geh\u00f6ren in <code>metadata/databases/</code></li> <li>Gruppiere Transformationsbl\u00f6cke logisch mit Annotationen</li> </ul>"},{"location":"hop_developer_guidelines/#4-modulare-pipelines","title":"\ud83d\udd01 4. Modulare Pipelines","text":"<ul> <li>Baue wiederverwendbare Subpipelines (z.\u202fB. <code>clean_customer_data.hpl</code>)</li> <li>Trenne Logik (Transformation) und Steuerung (Workflow) klar</li> <li>Ein Workflow = genau ein Jobziel (Load, Sync, Export etc.)</li> </ul>"},{"location":"hop_developer_guidelines/#5-testing-validierung","title":"\ud83e\uddea 5. Testing &amp; Validierung","text":"<ul> <li>Verwende \"Assert Rows\" zur Qualit\u00e4tskontrolle in Pipelines</li> <li>Automatisiere Tests \u00fcber CLI (<code>hop-run</code>) oder CI/CD-Tools</li> <li>F\u00fcge Validierungen am Anfang jeder Pipeline ein (Schema Check, Pflichtfelder)</li> </ul>"},{"location":"hop_developer_guidelines/#6-deployment-umgebungen","title":"\ud83d\udce6 6. Deployment &amp; Umgebungen","text":"<ul> <li>Definiere Umgebungen in <code>metadata/environment/</code> (<code>dev</code>, <code>test</code>, <code>prod</code>)</li> <li>Deployment per CLI (<code>hop-run</code>, <code>hop-server</code>) oder orchestriert (z.\u202fB. Airflow, GitLab CI)</li> <li>Halte Umgebungsvariablen konsistent und dokumentiert</li> </ul>"},{"location":"hop_developer_guidelines/#7-sicherheit-datenschutz","title":"\ud83d\udd12 7. Sicherheit &amp; Datenschutz","text":"<ul> <li>Maskiere sensible Felder direkt in Pipelines (z.\u202fB. Hash von E-Mails)</li> <li>Keine Passw\u00f6rter im Klartext \u2013 verwende Parameter + Secret Management</li> <li>DSGVO-relevante Pipelines klar kennzeichnen (<code>[PII]</code> im Namen)</li> </ul>"},{"location":"hop_developer_guidelines/#8-logging-monitoring","title":"\ud83d\udcc8 8. Logging &amp; Monitoring","text":"<ul> <li>Aktiviere detailliertes Logging (<code>log_level: Basic</code> oder <code>Detailed</code>)</li> <li>Leite Logs in zentrale Systeme (z.\u202fB. ELK, Grafana via Log-Dateien)</li> <li>Verwende eindeutige Run-IDs und schreibe Prozessmetadaten in Audit-Tabellen</li> </ul>"},{"location":"hop_developer_guidelines/#9-schnittstellen-integration","title":"\ud83d\udd17 9. Schnittstellen &amp; Integration","text":"<ul> <li>Eing\u00e4nge: Datenbank, Flat Files, Kafka, REST, S3</li> <li>Ausg\u00e4nge: Datenbank, S3, REST, Message Queue, Kafka</li> <li>Transformationsschritte klar dokumentieren (Schema, Erwartung, Typen) in Notes beschreiben</li> </ul>"},{"location":"hop_developer_guidelines/#10-dokumentation-versionierung","title":"\ud83d\udcda 10. Dokumentation &amp; Versionierung","text":"<ul> <li>Jede Pipeline &amp; Workflow braucht eine <code>.md</code> Beschreibung (Zweck, Input, Output, Besonderheiten)</li> <li>Versioniere Pipelines in Git (<code>.hpl</code>, <code>.hwf</code>, <code>.json</code>) \u2013 keine Bin\u00e4rdateien!</li> <li>Git-Konventionen:</li> <li>Feature-Branches: <code>feature/&lt;beschreibung&gt;</code></li> <li>Commits: <code>ADD</code>, <code>FIX</code>, <code>REFACTOR</code></li> </ul>"},{"location":"hop_developer_guidelines/#tipps","title":"\ud83e\udde0 Tipps","text":"<ul> <li>Nutze <code>Hop Gui</code> f\u00fcr visuelles Debugging \u2013 aber speichere sauber versionierte Dateien</li> <li>Erzeuge <code>preview</code>-Schritte f\u00fcr komplexe Transformationsketten</li> <li>Vermeide unn\u00f6tige Lookups \u2013 arbeite mit vorbereiteten Joins und Caching</li> </ul>"},{"location":"hop_developer_guidelines/#nutzliche-links","title":"\ud83d\udd17 N\u00fctzliche Links","text":"<ul> <li>Apache Hop Doku</li> <li>CLI Referenz</li> <li>GitHub Best Practices</li> </ul> <p>Diese Richtlinien stellen sicher, dass Apache Hop in einem Data Mesh-Umfeld skalierbar, teamf\u00e4hig und produktionstauglich eingesetzt wird.</p>"}]}